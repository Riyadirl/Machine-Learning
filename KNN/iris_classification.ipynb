{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "path = 'iris.csv'\n",
    "data = np.genfromtxt(path, delimiter = ',')\n",
    "np.random.shuffle(data)\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(data):\n",
    "   \n",
    "    train= 0.7\n",
    "    val = 0.15\n",
    "    test = 0.15\n",
    "\n",
    "  \n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "\n",
    "    # Use a for loop to split the dataset\n",
    "    for s in data:\n",
    "        prob = np.random.rand()\n",
    "        if prob <= train:\n",
    "            train_set.append(s)\n",
    "        elif prob > train and prob <= train + val:\n",
    "            val_set.append(s)\n",
    "        else:\n",
    "            test_set.append(s)\n",
    "\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data_preparation(data)\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classification(train, val, k):\n",
    "    #correct is a variable used to count how many data points in the validation set were correctly classified.\n",
    "    correct_count = 0\n",
    "\n",
    "    for V in val:\n",
    "        #distances is a list that will store pairs of training data points and their corresponding distances from the current validation data point V.\n",
    "        distances = []\n",
    "\n",
    "        # Calculate the Euclidean distance between V and all samples in the training set\n",
    "        for T in train:\n",
    "            # Calculate the distance\n",
    "            distance = np.linalg.norm(V[:-1] - T[:-1])  # Assuming the last element is the class label\n",
    "            distances.append((T, distance))\n",
    "\n",
    "        # Sort distances in ascending order\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "\n",
    "        # Take the first K samples\n",
    "        nearest_neighbors = distances[:k]\n",
    "\n",
    "        # Identify the detected class by taking the majority class from the K samples\n",
    "        classes = [neighbor[0][-1] for neighbor in nearest_neighbors]\n",
    "        detected_class = max(set(classes), key=classes.count)\n",
    "\n",
    "        # Check if the detected class is correct\n",
    "        if detected_class == V[-1]:\n",
    "            correct_count += 1\n",
    "\n",
    "    # Calculate the validation accuracy\n",
    "    validation_accuracy = (correct_count / len(val)) * 100\n",
    "    return validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classification for k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "validation_accuracy = knn_classification(train, val, K)\n",
    "print(f\"Validation Accuracy (k={K}): {validation_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
